{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 3000 - Assignment 5 (Group Practicum 1)\n",
    "\n",
    "**Student Name**: Group 34 (Noam Steiner Tomer, Shruti Bodhanampati, Ian Carr, Allison Lee)\n",
    "\n",
    "**Date**: 9/26/24\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "Submit this `ipynb` file to canvas.\n",
    "\n",
    "The `ipynb` format stores outputs from the last time you ran the notebook.  (When you open a notebook it has the figures and outputs of the last time you ran it too).  To ensure that your submitted `ipynb` file represents your latest code, make sure to give a fresh run `Kernel > Restart & Run All` just before uploading the `ipynb` file to Canvas.\n",
    "\n",
    "### Academic Integrity\n",
    "\n",
    "**Writing your homework is an individual effort.**  You may discuss general python problems with other students but under no circumstances should you observe another student's code which was written for this assignment, from this year or past years.  Pop into office hours or DM us in MS Teams if you have a specific question about your work or if you would like another pair of eyes or talk through your code.\n",
    "\n",
    "Don't forget to cite websites which helped you solve a problem in a unique way.  You can do this in markdown near the code or with a simple one-line comment. You do not need to cite the official python documentation.\n",
    "\n",
    "**Documentation / style counts for credit**  Please refer to the Pep-8 style, to improve the readability and consistency of your Python code. For more information, read the following article [How to Write Beautiful Python Code With PEP 8](https://realpython.com/python-pep8/) or ask your TA's for tips.\n",
    "\n",
    "**NOTE:<span style='color:red'> Write python expressions to answer ALL questions below and ensure that you use the `print()` function to display the output.</span>** Each question should be answered in a new code cell. For example, your solution for question 1.1 should be in a different code cell from your solution for question 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 — The DS Problem\n",
    "\n",
    "In your own words, explain the Data Science problem that you were given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem asks us to analyze airline on-time performance data for flights originating or departing from Arizona, Nevada, and California in 2019. The dataset includes flight dates, carrier codes, departure and arrival times, delays, and distances. The objective is to evaluate flight delays, identify patterns or trends in airline performance, and understand whether certain factors like the state, airport, or carrier contribute to longer delays. The analysis aims to provide insights that could help improve flight efficiency and on-time performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 — Prepare the Data (10 pts)\n",
    "\n",
    "Evaluate the dataset to determine if ALL variables are represented in their expected type. Convert variables to suitable data types (if needed).\n",
    "Analyze the data to determine what preprocessing steps are needed. Perform the required data preparation steps. At a minimum, ensure that you handle both missing and invalid values. Justify the approach that is taken to prepare the data for analysis.\n",
    "At a high level explain what the dataset is about and provide at least three visualizations to support your explanation of DS problem from Question 1. Note: this explanation and visualizations should be relevant to the DS Problem in question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2019_ONTIME_REPORTING_FSW.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# importing data into a dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2019_ONTIME_REPORTING_FSW.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2019_ONTIME_REPORTING_FSW.csv'"
     ]
    }
   ],
   "source": [
    "# importing data into a dataframe\n",
    "df = pd.read_csv('2019_ONTIME_REPORTING_FSW.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# removing duplicate data rows from the dataframe\n",
    "df.drop_duplicates(inplace=True)\n",
    "# seeing how many rows remain in the dataframe\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there were no duplicate rows removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking frequency of missing values of columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indentifying the maximum number of missing variables in a row\n",
    "df.isnull().sum(axis=1).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are at maximum 6 variables missing at once from a row\n",
    "# looking at one of the rows where all six variables are missing\n",
    "df.loc[33745]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks to be a widespread issue meaning that the best method to handle this missing data is deletion. I will remove all rows with NaN in the ELAPSED_TIME column as that is the highest column with the most number of variables deleted, tied with the ARR_DELAY column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# should removed 31884 rows where there was a NaN value in the ELAPSED_TIME col\n",
    "df = df.drop(df[df['ELAPSED_TIME'].isnull()].index)\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal number of rows minus the new number of rows should equate to 31884\n",
    "1897503-1865619"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing data type and non-null count of each variable in the dataframe\n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All null rows were removed with the removal of NaN values from ELAPSED_TIME as the issue was widespread throughout the data. \n",
    "\n",
    "When looking at the variables represented in the dataframe, one can see that all categorical variables (CARRIER_CODE, TAIL_NUM, ORIGIN, ORIGIN_ST, DEST, DEST_ST) are of the object type, so they should be cast to the \"category\" type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CARRIER_CODE'] = df['CARRIER_CODE'].astype('category')\n",
    "df['TAIL_NUM'] = df['TAIL_NUM'].astype('category')\n",
    "df['ORIGIN'] = df['ORIGIN'].astype('category')\n",
    "df['ORIGIN_ST'] = df['ORIGIN_ST'].astype('category')\n",
    "df['DEST'] = df['DEST'].astype('category')\n",
    "df['DEST_ST'] = df['DEST_ST'].astype('category')\n",
    "\n",
    "# checking the data types to make sure they are updated \n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the DEP_TIME, DEP_DELAY, ARR_TIME, ARR_DELAY, and ELAPSED_TIME into datetime values.\n",
    "\n",
    "Before we do this however we need to check that all the values in the columns we're going to convert to datetime have integer values since we need to convert the floats to integers during the datetime conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_values_int_like(colname):\n",
    "    num_non_integers = df[colname].apply(lambda x: not x.is_integer() if pd.notnull(x) else False).sum()\n",
    "    print(f'{colname}: {num_non_integers}')\n",
    "\n",
    "cols_to_convert = ['DEP_TIME', 'DEP_DELAY', 'ARR_TIME', 'ARR_DELAY', 'ELAPSED_TIME']\n",
    "print(\"Number of non-integer values per column:\")\n",
    "for col in cols_to_convert:\n",
    "    check_all_values_int_like(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all columns have integer values.\n",
    "\n",
    "The columns DEP_TIME and ARR_TIME can be turned into datetime values by turning the integers into a time, e.g. 823 -> 08:23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_float_to_time_string(time_float):\n",
    "    time_int_str = str(int(time_float)).zfill(4)\n",
    "    hours, minutes = (time_int_str[:len(time_int_str)//2], time_int_str[len(time_int_str)//2:])\n",
    "    hours = '00' if hours == '24' else hours\n",
    "    return hours + ':' + minutes\n",
    "\n",
    "df['DEP_TIME'] = pd.to_datetime(df['DEP_TIME'].apply(time_float_to_time_string), format='%H:%M')\n",
    "df['ARR_TIME'] = pd.to_datetime(df['ARR_TIME'].apply(time_float_to_time_string), format='%H:%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DEP_DELAY, ARR_DELAY, and ELAPSED_TIME columns have values of type float can be converted to timedelta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mins_float_to_time_string(mins_float):\n",
    "    mins_int = int(mins_float)\n",
    "    hours, minutes = divmod(mins_int, 60)\n",
    "    return f'{hours:02}:{minutes:02}:00'\n",
    "\n",
    "df['DEP_DELAY'] = pd.to_timedelta(df['DEP_DELAY'].apply(mins_float_to_time_string))\n",
    "df['ARR_DELAY'] = pd.to_timedelta(df['ARR_DELAY'].apply(mins_float_to_time_string))\n",
    "df['ELAPSED_TIME'] = pd.to_timedelta(df['ELAPSED_TIME'].apply(mins_float_to_time_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data types to make sure they are updated\n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last data type to be addressed is the first column, the FL_DATE column. This column would best be represented by being represented as a DateTime variable in the data frame. This is a quick fix which will be taken care of in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'FL_DATE' to datetime\n",
    "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data types to make sure they are updated \n",
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables have been updated to their correct types and missing data has been handled. It is now time to look for invalid data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing summary stats of all rows\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of states to check against\n",
    "valid_states = ['CA', 'NV', 'AZ']\n",
    "\n",
    "# check if values in columns ORIGIN_ST OR DEST_ST are in valid_states list\n",
    "df['CHECKING_ST'] = df['ORIGIN_ST'].isin(valid_states) | df['DEST_ST'].isin(valid_states)\n",
    "df['CHECKING_ST'].value_counts()\n",
    "\n",
    "# drop column since it is only being used here\n",
    "df.drop(['CHECKING_ST'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each flight in the data either took off or left from one of the states ['CA', 'NV', 'AZ']\n",
    "# checking if any flights left and landed at the same airport\n",
    "count = (df['ORIGIN'].astype(str) == df['DEST'].astype(str)).sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no flights left and landed at the same airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a clearer understanding of the dataset, we will create 3 visualizations that will help us better understand the overall flight patterns in Arizona, Nevada, and California. The visualizations will provide a broader context for the dataset, which can help us understand flight efficiency and on-time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Total Number of Flights per State (AZ, NV, CA)\n",
    "state_flight_counts = df['ORIGIN_ST'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(state_flight_counts.index, state_flight_counts.values, color=['blue', 'green', 'orange'])\n",
    "plt.title('Total Flights per State (AZ, NV, CA)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Flights')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# 2. Flight Distance Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df['DISTANCE'], bins=20, color='purple', edgecolor='black')\n",
    "plt.title('Flight Distance Distribution')\n",
    "plt.xlabel('Distance (miles)')\n",
    "plt.ylabel('Number of Flights')\n",
    "plt.show()\n",
    "\n",
    "# 3. Carrier Market Share by Number of Flights\n",
    "carrier_flight_counts = df['CARRIER_CODE'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(carrier_flight_counts.head(5), labels=carrier_flight_counts.index[:5], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Market Share of Top 5 Carriers by Flights')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualizations help us understand the dataset and the homework. By examining the total flight volume per state, we gain insight into the distribution of air traffic across Arizona, Nevada, and California, which is essential for contextualizing the delays analyzed later. A higher concentration of flights, as seen in California, may lead to more congestion and, consequently, a higher potential for delays. The flight distance distribution is also relevant to our analysis, as it helps us understand the types of flights in the dataset. This is essential when considering how flight distance may affect delays and how we interpret the performance of flights departing from or arriving in these states. Lastly, the carrier market share helps us see which airlines are responsible for most flights in the dataset, which becomes particularly relevant when we assess the delay patterns for specific carriers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 — (20 points)\n",
    "\n",
    "Explore patterns in the region i.e. the 3 states (AZ, NV, CA). Determine which region has the most air traffic. Comment on your findings and visualize the results. Note: it's important to explain your rational for determining what is \"the most air traffic\". For example, are you evaluating the frequency of flights or total flight time. Justify your response.\n",
    "For each region i.e. the 3 states (AZ, NV, CA), analyze the most popular outbound/destination airports. For example, if a flight originated in CA (at any of its airports), where do they often go? Comment on your findings and visualize the results for the top 5 destinations.\n",
    "For each region, calculate the proportion* of flights from each airline/operator. Visualize the top 10 results. Explain the results.\n",
    "*Hint: Don't forget that proportion is an explicit requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.1: We define \"the most air traffic\" to be the highest number of flights, both arriving and departing. The reason we use this definition is that \"air traffic\" as a term means the number of aircraft that are airborne in a given airspace at a given time. A good proxy for this is the number of flights arriving to and departing from the region, since the more arrivals and departures there are the more airplanes are in the sky at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures = df['ORIGIN_ST'].value_counts()\n",
    "arrivals = df['DEST_ST'].value_counts()\n",
    "\n",
    "# combine the counts of departures and arrivals for each state to get total\n",
    "total_air_traffic = departures.add(arrivals, fill_value=0)\n",
    "\n",
    "filtered_total_air_traffic = total_air_traffic[total_air_traffic.index.isin(['CA', 'NV', 'AZ'])]\n",
    "filtered_total_air_traffic.plot(kind='bar', title='Total Arrivals and Departures per State', xlabel='State', ylabel='Total Arrivals and Departures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see California has the most air traffic by far. It has almost twice as many arrivals and departures as the other two states combined. This is not surprising since California takes in almost all flights coming to the west coast of the US, including international flights crossing the Pacific Ocean as well as some direct flights to Europe which increases its total arrivals and departures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.2: Now we will look at the most popular inbound/outbound airports in each of the three states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['AZ', 'NV', 'CA']\n",
    "filtered_df_states = df[df['ORIGIN_ST'].isin(states)]\n",
    "\n",
    "# Count number of flights to each destination\n",
    "popular_destinations = (\n",
    "    filtered_df_states.groupby(['ORIGIN_ST', 'DEST'], observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='num_flights')\n",
    ")\n",
    "\n",
    "# Get top 5 most popular destinations\n",
    "top_destinations = popular_destinations.groupby('ORIGIN_ST', observed=True)[['DEST', 'num_flights']].apply(\n",
    "    lambda x: x.nlargest(5, 'num_flights')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "for i, state in enumerate(states):\n",
    "    state_data = top_destinations[top_destinations['ORIGIN_ST'] == state]\n",
    "    axes[i].bar(state_data['DEST'], state_data['num_flights'], color='skyblue')\n",
    "    axes[i].set_title(f'Top 5 Destinations from {state}')\n",
    "    axes[i].set_xlabel('Destination Airports')\n",
    "    axes[i].set_ylabel('Number of Flights')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for Arizona and Nevada one of the most popular destinations is LAX, and that for California the two most popular destinations are Pheonix and Las Vegas. Interestingly for both Nevada and California one of the most popular destinations is San Francisco, meaning that there are a lot of California flights that never leave the state. Seattle is also a popular destination for all 3 states. One interesting detail is that one of the most popular destinations from Arizona is to the Dallas Fort-Worth airport, which isn't seen in any of the other states. The fact that LAX is present in the top 5 destinations from California suggests that similarly to the flights to San Francisco, there are a lot of flights internal to California that take passengers between different parts of California. Denver is also a popular destination from both Arizona and Nevada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.3: Now we will look at the top 10 airlines by proportion of flights in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_to_analyze = ['AZ', 'NV', 'CA']\n",
    "filtered_df_states = df[df['ORIGIN_ST'].isin(states_to_analyze)]\n",
    "\n",
    "# Group by origin state and carrier to count the number of flights per carrier\n",
    "flight_counts = (\n",
    "    filtered_df_states.groupby(['ORIGIN_ST', 'CARRIER_CODE'], observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='num_flights')\n",
    ")\n",
    "\n",
    "# Calculate total flights per state\n",
    "total_flights_per_state = flight_counts.groupby('ORIGIN_ST', observed=True)['num_flights'].sum()\n",
    "\n",
    "def calculate_proportion(row):\n",
    "    return row['num_flights'] / float(total_flights_per_state.loc[row['ORIGIN_ST']])\n",
    "\n",
    "flight_counts['proportion'] = flight_counts.apply(calculate_proportion, axis=1)\n",
    "\n",
    "top_airlines = flight_counts.groupby('ORIGIN_ST', observed=True)[['CARRIER_CODE','proportion']].apply(\n",
    "    lambda x: x.nlargest(10, 'proportion')\n",
    ").reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "for i, state in enumerate(states_to_analyze):\n",
    "    state_data = top_airlines[top_airlines['ORIGIN_ST'] == state]\n",
    "    axes[i].bar(state_data['CARRIER_CODE'], state_data['proportion'], color='skyblue')\n",
    "    axes[i].set_title(f'Top 10 Carriers by Proportion in {state}')\n",
    "    axes[i].set_xlabel('Carrier Code')\n",
    "    axes[i].set_ylabel('Proportional of Flights')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can see that Southwest Airlines (WN) is the most common carrier in all 3 states. This makes sense because Southwest Airlines' main business is having frequent flights around the southwestern US. An interesting detail is that in Arizona, American Airlines is almost tied with Southwest Airlines in terms of proportion of flights but that it is much further behind in Nevada and California. We see Delta Airlines is present in the middle of the top 10 carriers by proportion for all 3 states. This makes sense because Delta is spread much wider in terms of flights, which will decrease the amount of presence they have in any particular area. Interestingly Sky West Airlines (OO) is very present in Arizona and California, but is much less present in Nevada. Conversly, United Airlines is much more present in California and Nevada but not Arizona. We can see Mesa Airlines (YV) is present in both Arizona and California, but much more so in Arizona. This makes sense Mesa Airlines is a regional carrier based in Phoenix, Arizona. There are some other carriers like Frontier Airlines (F9), Jetblue (B6), and Allegiant (G4) which make up a much smaller proportion of flights in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 — (20 points)\n",
    "\n",
    "Explore the carriers within the dataset and demonstrate at least TWO (2) patterns that you identified.\n",
    "Analyze the minimum, maximum and average arrival and departure delays for each Airline/Carrier. Visualize the top 10 results. Explain the patterns in the delays and demonstrate: 1) which carriers are more prone to flight delays and 2) are the delays occurring more frequently in certain regions**?\n",
    "**Tip: it is important to analyze the airlines across multiple airports and/or regions in order to conclude that they have a pattern of being late."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 - We will begin question 4. by exploring the carriers within the dataset and demonstrating at least 2 patterns that we identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by carrier and calculate average arrival and departure delays\n",
    "carrier_delays = df.groupby('CARRIER_CODE', observed=True).agg(\n",
    "    avg_arr_delay=('ARR_DELAY', 'mean'),\n",
    "    avg_dep_delay=('DEP_DELAY', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# sort by average delays to identify carriers with the highest delays\n",
    "top_carriers_by_arr_delay = carrier_delays.sort_values(by='avg_arr_delay', ascending=False).head(10)\n",
    "top_carriers_by_dep_delay = carrier_delays.sort_values(by='avg_dep_delay', ascending=False).head(10)\n",
    "\n",
    "def format_timedelta(td):\n",
    "    total_seconds = td.total_seconds()\n",
    "    minutes = int(total_seconds // 60)\n",
    "    seconds = round(total_seconds % 60, 3)\n",
    "    return f'{minutes}:{seconds:06.3f}'\n",
    "\n",
    "def print_carrier_delay(row):\n",
    "    carrier_code = row['CARRIER_CODE']\n",
    "    arr_delay = row['avg_arr_delay']\n",
    "    dep_delay = row['avg_dep_delay']\n",
    "    print(f\"{carrier_code}        {format_timedelta(arr_delay)}        {format_timedelta(dep_delay)}\")\n",
    "\n",
    "# Apply this function to each row in top_carriers_by_arr_delay and top_carriers_by_dep_delay\n",
    "print(\"Top carriers by Arrival Delay (minutes):\")\n",
    "print(\"Carrier   Arrival Delay   Departure Delay\")\n",
    "top_carriers_by_arr_delay.apply(print_carrier_delay, axis=1)\n",
    "\n",
    "print(\"\\nTop carriers by Departure Delay (minutes):\")\n",
    "print(\"Carrier   Arrival Delay   Departure Delay\")\n",
    "# assign to prevent printing of leftover values since this is the last expression in the cell\n",
    "_ = top_carriers_by_dep_delay.apply(print_carrier_delay, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we explore the carriers within the dataset and identify two significant patterns related to flight delays. These patterns are crucial for understanding the performance of different airlines and helping stakeholders make informed decisions regarding airline operations, customer service improvements, and regional airport management. One important pattern is that certain carriers, such as Frontier Airlines (F9) and JetBlue Airways (B6), consistently experience higher delays than others. Both airlines rank among the top 10 regarding average arrival and departure delays across multiple regions. This is crucial because it indicates potential operational inefficiencies or challenges in maintaining on-time performance, affecting customer satisfaction and airline reliability. Identifying carriers with persistent delays helps airports and airlines target improvement efforts to reduce delays and enhance service quality. Another key pattern is that delays for the same carrier vary by region. For example, airlines like SkyWest Airlines (OO) have higher average delays in certain states like Arizona (AZ) than others. This suggests that regional factors, such as airport congestion or weather conditions, may contribute to higher delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 - Analyze the minimum, maximum, and average arrival and departure delays for each Airline/Carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by carrier to calculate minimum, maximum, and average for both arrival and departure delays\n",
    "carrier_delay_stats = df.groupby('CARRIER_CODE', observed=True).agg(\n",
    "    min_arr_delay=('ARR_DELAY', 'min'),\n",
    "    max_arr_delay=('ARR_DELAY', 'max'),\n",
    "    avg_arr_delay=('ARR_DELAY', 'mean'),\n",
    "    min_dep_delay=('DEP_DELAY', 'min'),\n",
    "    max_dep_delay=('DEP_DELAY', 'max'),\n",
    "    avg_dep_delay=('DEP_DELAY', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "carrier_delay_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step analyzes the minimum, maximum, and average delays for arrival and departure times across all carriers. Our analysis shows how each airline performs under different conditions. For example, while some airlines may have occasional extreme delays (as seen from the maximum delay), others might consistently have longer average delays, which is more concerning for operational efficiency and customer experience. Analyzing the minimum delays helps us understand whether airlines can also perform efficiently in some instances. The combination of minimum, maximum, and average delays paints a clearer picture of each carrier's best and worst-case scenarios. This data is essential for airlines to identify areas for improvement, allowing them to reduce their overall delay times and maintain a more consistent on-time performance. Additionally, airports and regulatory authorities can use this information to hold airlines accountable for their punctuality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 - Visualize the top 10 results for delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sort the data by average arrival and departure delays for visualization\n",
    "top_10_carriers_by_arr_delay = carrier_delay_stats.sort_values(by='avg_arr_delay', ascending=False).head(10)\n",
    "top_10_carriers_by_dep_delay = carrier_delay_stats.sort_values(by='avg_dep_delay', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# average arrival delays\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(top_10_carriers_by_arr_delay['CARRIER_CODE'], top_10_carriers_by_arr_delay['avg_arr_delay'], color='blue')\n",
    "plt.title('Top 10 Carriers by Average Arrival Delay')\n",
    "plt.xlabel('Average Arrival Delay (minutes)')\n",
    "plt.ylabel('Carrier')\n",
    "\n",
    "# average departure delays\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(top_10_carriers_by_dep_delay['CARRIER_CODE'], top_10_carriers_by_dep_delay['avg_dep_delay'], color='green')\n",
    "plt.title('Top 10 Carriers by Average Departure Delay')\n",
    "plt.xlabel('Average Departure Delay (minutes)')\n",
    "plt.ylabel('Carrier')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we visualize the top 10 carriers based on their average arrival and departure delays. By presenting the data graphically, we could spot patterns, such as which airlines consistently rank high in delays, making it easier to communicate insights to stakeholders. The bar charts help reveal that certain airlines, like Frontier Airlines (F9) and JetBlue Airways (B6), appear frequently among the top delayed carriers. This visual representation is important because it simplifies the data, making it more accessible and easier to understand. It also allows airlines to benchmark their performance against competitors and prioritize strategies to reduce delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 — (30 points)\n",
    "\n",
    "Evaluate which airlines have the best performance in the region. Display the top 10 airlines.\n",
    "Note: it's important to explain your rational for determining \"the best performance\".\n",
    "For each airline, ensure that you calculate their total flight hours for each month. Explain and visualize the results for the top 10 airlines. \n",
    "Hint: the total flight hours is not equivalent to the frequency of flights, and ensure that you display the total hours and not the total minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.1: Evaluate which airlines have the best performance in the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to determine the best performance of an airline would be to determine the average delay time for each flight from that airline. To do so, it makes most sense to create a new column averaging the departure delay and arrival delay to find the overall delay for each flight. From there, I will divide the total time of all delays for an airline by the total number of flights to find the average time each flight is delayed. This data will show how long on average customers will wait because of delays by airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new column for overall delay\n",
    "df['OVERALL_DELAY']=(df['ARR_DELAY']+df['DEP_DELAY'])/2\n",
    "\n",
    "# grouping df and finding avg overall delay\n",
    "avg_delay_time = pd.DataFrame(df.groupby('CARRIER_CODE', observed=False)['OVERALL_DELAY'].apply(lambda x: x.sum()/x.count())).sort_values(by='OVERALL_DELAY').head(10).reset_index()\n",
    "avg_delay_time['AVG_DELAY'] = round(avg_delay_time['OVERALL_DELAY'] / np.timedelta64(1, 'm'), 2)\n",
    "avg_delay_time = avg_delay_time.drop(columns='OVERALL_DELAY')\n",
    "print(avg_delay_time)\n",
    "\n",
    "# display the top ten airlines with the shortest average number of flights\n",
    "plt.bar(avg_delay_time['CARRIER_CODE'], avg_delay_time['AVG_DELAY'], color='red')\n",
    "plt.ylabel('Number of Minutes Delayed')\n",
    "plt.xlabel('Airline')\n",
    "plt.title('Average Flight Delay Time by Airline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is showing that the airline with the least amount of average delay time is Southwest Airlines. The graph shows that Southwest's average delay time was 10.47 minutes. The following airlines with the least amount of time were Delta with an average delay time of 10.65 minutes and then Envoy Air with an average delay time of just 10.84 minutes.\n",
    "\n",
    "One additional measure can be to find the proportion of delayed flights for each airline. This will help show which airlines have the lowest frequency of delayed flights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding proportion of flights delayed\n",
    "df['DELAYED %'] = np.where(df['OVERALL_DELAY'] > pd.Timedelta(0), 1, 0)\n",
    "delay_percent = pd.DataFrame(df.groupby('CARRIER_CODE', observed=False)['DELAYED %'].apply(lambda x: round(x.sum()/x.count() * 100, 2))).sort_values(by='DELAYED %').head(10).reset_index()\n",
    "print(delay_percent)\n",
    "\n",
    "# display the top ten airlines with the leat percentage of flights delayed\n",
    "plt.bar(delay_percent['CARRIER_CODE'], delay_percent['DELAYED %'], color='orange')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Airline')\n",
    "plt.title('Percentage of Flights Delayed by Airline')\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is showing that the airline with the lowest proportion of flights delayed is Skywest airlines. The graph shows that Skywest's proportion of flights delayed was just under 36%. The following airlines with the lowest proportions were Republic Airways with 36.36% of flights being delayed and then ExpressJet Airlines with roughly 38.30% of flights being delayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5.2: Now we will look at the total monthly flight hours for each airline, for the top 10 airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states_to_analyze = ['AZ', 'NV', 'CA']\n",
    "filtered_df_states = df[df['ORIGIN_ST'].isin(states_to_analyze)]\n",
    "filtered_df_states = filtered_df_states.copy()\n",
    "filtered_df_states['YEAR_MONTH'] = filtered_df_states['FL_DATE'].dt.to_period('M')\n",
    "filtered_df_states['FLIGHT_HOURS'] = filtered_df_states['ELAPSED_TIME'].dt.total_seconds() / 3600\n",
    "\n",
    "monthly_carrier_hours = filtered_df_states.groupby(['YEAR_MONTH', 'CARRIER_CODE'], observed=True)['FLIGHT_HOURS'].sum().reset_index()\n",
    "\n",
    "top_carriers_per_month = monthly_carrier_hours.groupby('YEAR_MONTH', observed=True)[['YEAR_MONTH', 'CARRIER_CODE', 'FLIGHT_HOURS']].apply(\n",
    "    lambda x: x.nlargest(10, 'FLIGHT_HOURS')\n",
    ").reset_index(drop=True)\n",
    "\n",
    "months = top_carriers_per_month['YEAR_MONTH'].unique()\n",
    "\n",
    "for month in months:\n",
    "    monthly_data = top_carriers_per_month[top_carriers_per_month['YEAR_MONTH'] == month]\n",
    "    monthly_data = monthly_data.sort_values(by='FLIGHT_HOURS', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(monthly_data['CARRIER_CODE'], monthly_data['FLIGHT_HOURS'], color='skyblue')\n",
    "    plt.title(f'Top 10 Carriers by Total Flight Hours in {month}', fontsize=16)\n",
    "    plt.xlabel('Carrier Code', fontsize=12)\n",
    "    plt.ylabel('Total Flight Hours', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Southwest Airlines (WN) have the most total flight hours every month, followed by American Airlines and United Airlines. This corroborates our findings in Q3 where we found that Southwest Airlines was the top airline by proportion of flights for all 3 states. It makes sense that the carrier with the most flights will have the most flight hours in a particular region. The total flight hours per carrier are extremely consistent across months as well, which makes sense considering the fact that flight scheduling and routes are also very consistent over time. An interesting detail is that in February the total flight hours for Southwest Airlines is lower than usual by about 1000 flight hours. For other airlines the number of flight hours also dropped, but not to the same extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 — (20 points)\n",
    "\n",
    "Select any THREE (3) aircraft***, and explore the data to analyze the following for each aircraft:\n",
    "\n",
    "Indicate which airline operates the selected aircraft and where it often travels.\n",
    "Arrival and departure delays at the airports where it traveled.\n",
    "Create summary statistics on the distance traveled.\n",
    "Analyze all the results in 6.1-6.3 to identify any patterns that are evident. Explain your findings and visualize ALL results.\n",
    "***Note: the TAIL_NUM can help you to identify each unique aircraft.\n",
    " \n",
    "\n",
    "Question 6 — Summary\n",
    "\n",
    "Summarize the overall analysis and share THREE (3) key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the tail numbers (Max, median, min freq.)\n",
    "# aggregating the frequencies\n",
    "tails_counts = df['TAIL_NUM'].value_counts()\n",
    "q1 = np.quantile(tails_counts, 0.25)\n",
    "q2 = np.quantile(tails_counts, 0.5)\n",
    "q3 = np.quantile(tails_counts, 0.75) # wanted to use the max value (q4) instead of q3\n",
    "max_val = tails_counts.max()\n",
    "#median_val = tails_counts.median()\n",
    "#min_val = tails_counts.min()\n",
    "\n",
    "max_flights_plane = tails_counts[tails_counts == max_val].index[0]\n",
    "median_flights_plane = tails_counts[tails_counts == q2].index[0]\n",
    "min_flights_plane = tails_counts[tails_counts == q1].index[0]\n",
    "print(f\"Aircraft with max flight freq.: {max_flights_plane} {max_val} total flights (in the given time frame)\")\n",
    "print(f\"Aircraft with median flight freq.: {median_flights_plane} {q2} total flights\")\n",
    "print(f\"Aircraft with flight freq. in the 1st quartile: {min_flights_plane} {q1} total flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tails_counts.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Indicate which airline operates the selected aircraft and where it often travels.\n",
    "# OO carrier code is for skywest, UA for united, and DL for delta airlines\n",
    "# 5 most often destinations\n",
    "\n",
    "def tails_analyze(tail_num):\n",
    "    aircraft_info = df.loc[df['TAIL_NUM'] == tail_num]\n",
    "    print('Carrier code: ' + aircraft_info.iloc[0, 1])\n",
    "    aircraft_freq_dest = aircraft_info['DEST'].value_counts().head(5)\n",
    "    aircraft_freq_dest.plot(kind='bar', title=tail_num + ' most travelled', xlabel='Destination', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tails_analyze(max_flights_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like N706SK travelled primarily in round trips from Pheonix, Arizona to LAX. It did have some flights to Yuma International Airport in Yuma, Arizona, Fresno Yosemite International Airport in Fresno, California, and to San Luis Obispo County Regional Airport in California. Overall it looks like it travelled for many trips in the southwestern US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tails_analyze(median_flights_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like N491UA travelled from San Francisco to other airports in the southwestern US like Denver and Las Vegas, but also to a couple further airports like George Bush International Airport in Houston, Texas, and O'Hare International Airport in Chicago, Illinois. Overall it looks to have a more mixed flight history than N706SK with some longer flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tails_analyze(min_flights_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like N342NW traveled primarily to Los Angeles, then around to Dallas and Phoenix and other southwestern areas. Whats interesting is that one of its top destinations is Minneapolis -- one of the further east airports we're observing from these three charts. Additionally Seattle is one of the further north spots of these top destinations. It seems to triangulate major spots around the general (mid)west."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously exploring the minimum freq. flight:\n",
    "It seems like N821NW had only one flight. Potentially it only had 1 flight because it may have been undergoing deep maintenance that all planes go through at certain intervals. N821NW is an Airbus A330 which had been manufactured in 2007 and had been flying with Delta Airlines since 2009 (See link below for source). This makes 2019 a decade since it began flight operations with Delta Airlines, which potentially is a lifespan threshold for deep maintenance. Deep maintenance operations require taking the plane almost fully apart and then rebuilding it, and given the complexity of large wide-body commercial jets like the A330 it would not be suprising if maintenance takes many months.\n",
    "\n",
    "Link: https://registry.faa.gov/AircraftInquiry/Search/NNumberResult?nNumberTxt=821NW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Arrival and departure delays at the airports where it traveled.\n",
    "\n",
    "def arr_dep_delay_plot(tail_num):\n",
    "    aircraft_info = df.loc[df['TAIL_NUM'] == tail_num]\n",
    "    aircraft_freq_dest = aircraft_info['DEST'].value_counts().head(5)\n",
    "    airports = aircraft_freq_dest.index[:5]\n",
    "\n",
    "    # get the average arrival and departure delay times for frequented airports\n",
    "    \n",
    "    def arr_delay_mean(val):\n",
    "        airport = aircraft_info.loc[aircraft_info['DEST'] == airports[val]]\n",
    "        return airport['ARR_DELAY'].mean().total_seconds()/60\n",
    "    \n",
    "    def dep_delay_mean(val):\n",
    "        airport = aircraft_info.loc[aircraft_info['DEST'] == airports[val]]\n",
    "        return airport['DEP_DELAY'].mean().total_seconds()/60\n",
    "        \n",
    "    avg_arrival_delay = [arr_delay_mean(0), arr_delay_mean(1), arr_delay_mean(2), arr_delay_mean(3), arr_delay_mean(4)]\n",
    "    avg_depart_delay = [dep_delay_mean(0), dep_delay_mean(1), dep_delay_mean(2), dep_delay_mean(3), dep_delay_mean(4)]\n",
    "        \n",
    "    x_axis = np.arange(len(airports))\n",
    "    \n",
    "    plt.bar(x_axis - 0.2, avg_arrival_delay, 0.4, label='Arr. delays')\n",
    "    plt.bar(x_axis + 0.2, avg_depart_delay, 0.4, label='Dep. delays')\n",
    "    \n",
    "    plt.xticks(x_axis, airports)\n",
    "    plt.xlabel('Destination')\n",
    "    plt.ylabel('Arr/Dep Delay Time (min)')\n",
    "    plt.title(tail_num + ' Airports and Delays')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dep_delay_plot(max_flights_plane)\n",
    "arr_dep_delay_plot(median_flights_plane)\n",
    "arr_dep_delay_plot(min_flights_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the number of arrival and departure delays for the different aircraft vary somewhat between different airports. Clearly the aircraft with the minimum number of flights N821NW didn't have any meaningful data for airports and delays due to the fact that it had done only one flight. Interestingly it seems like smaller airports like Yuma International Airport and San Luis Obispo Regional Airport had more delays that larger airports. This is potentially due the air traffic controllers and other airport staff in the larger airports having more experience and thus more efficiency in direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Create summary statistics on the distance traveled.\n",
    "\n",
    "# summary: max, med/mean, min, range, etc...\n",
    "\n",
    "def distance_summary(tail_num):\n",
    "    aircraft_info = df.loc[df['TAIL_NUM'] == tail_num]\n",
    "    max_dist = aircraft_info['DISTANCE'].max()\n",
    "    mean_dist = aircraft_info['DISTANCE'].mean()\n",
    "    min_dist = aircraft_info['DISTANCE'].min()\n",
    "    standev = np.std(aircraft_info['DISTANCE'])\n",
    "    range_dist = max_dist-min_dist\n",
    "    print('Summary of '+tail_num+' distance traveled in miles:')\n",
    "    print('Total miles: '+str(aircraft_info['DISTANCE'].sum()))\n",
    "    print('Per trip:')\n",
    "    print('Max: '+str(max_dist)+', mean: '+str(mean_dist)+', min: '+str(min_dist)+', range: '+str(range_dist)+', std: '+str(standev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_summary(max_flights_plane)\n",
    "distance_summary(median_flights_plane)\n",
    "distance_summary(min_flights_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.4: Analyze all the results in 6.1-6.3 to identify any patterns that are evident. \n",
    "Explain your findings and visualize ALL results. Summarize the overall analysis and share THREE (3) key findings.\n",
    "\n",
    "Summary: The major airports of the west for these three planes on average appear to be PHX, LAX, and SFO. Larger airports (on average) are more efficient. Airlines do have major parts to play in delays. Shorter trips have a higher turnaround and accumulations of miles.\n",
    "\n",
    "1. I found that the most frequently flying aircraft would on average travel less in distance versus the less frequently fying aircraft. The airline it belonged to was a regional airline (skywest), and it fly short distances between airports in Arizona and California. This makes sense because it makes many short trips (faster turn around) just in the area. \n",
    "\n",
    "2. There was an interesting point in arrival and departure delays, where they would typically have a higher delay in arrival v. departure except for in Las Vegas and Chicago, where aircraft N491UA (median in flight frequency) would experience a longer delay in departure instead. Additionally, Minneapolis appears to have a more major gap. Reasons why may vary, there are many extraneous variables at play; Possibly by an outlying event, or tardiness of flight crew, or perhaps having a larger intake volume of flights than manageable. Lastly, whats interesting was how Las Vegas had the highest avg delay time of 25 min (in the 3 aircrafts analyzed) -- maybe because it's a more condensed area.\n",
    "\n",
    "3. Airlines have major roles in delays, as showcased by fig 5.1, where the proportion of the flights delayed by Skywest are low. Then comparing the discrepency between N706SK's LAX delays vs N342NW's LAX delays fig 6.2, it suggests that the airline (Delta) may have a part to play in the greater gap (although PHX seems to have a similar delay measuring between the two.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful resources \n",
    "Don't forget to cite websites which helped you solve a problem in a unique way.  You can do this in markdown near the code or with a simple one-line comment inside the code cell, or you can list them below. \n",
    "\n",
    "You do not need to cite the official python documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
